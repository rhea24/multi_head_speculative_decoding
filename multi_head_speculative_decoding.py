# -*- coding: utf-8 -*-
"""multi_head_speculative_decoding.ipynb

Automatically generated by Colab.

PROJECT: SPECULATIVE DECODING ON A CUSTOM TRANSFORMER 

PART 1: ARCHITECTURE 
- Implemented Multi-Head Attention, FeedForward, and Transformer Blocks
- Replicates GPT-2 architecture logic from first principles

PART 2: SPECULATIVE DECODING
- Implements Blockwise Parallel Decoding
- Freezes pre-trained transformer to train only auxiliary heads
- Acclerated inference by 1.4x

Original file is located at
    https://colab.research.google.com/drive/14tbzPTLCmauwxL_fJ740c0jl5csbD9Hm

Credits & References
- Andrej Karpathy's "Let's reproduce GPT-2 (124M)" from scratch
"""

import torch
import torch.nn as nn
from torch.nn import functional as F
import time
import math

!pip install tiktoken datasets
import tiktoken
from datasets import load_dataset

# Configs / Hyperparams
BATCH_SIZE = 16
BLOCK_SIZE = 64 # context window
LEARNING_RATE = 2.5e-4
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

N_EMBD = 128
N_HEAD = 4
N_LAYER = 4
DROPOUT = 0.2

NUM_SPEC_HEADS = 3 # number of future tokens to predict
DECAY_COEFF = 0.8 # loss weighting for future tokens

PRETRAIN_STEPS = 2000
HEAD_TRAIN_STEPS = 2000

print("Loading TinyStories (subset)...")
dataset = load_dataset("roneneldan/TinyStories", split="train[:1%]")
raw_text = '\n'.join(dataset['text'])

print("Tokenizing with GPT-2 BPE...")
enc = tiktoken.get_encoding("gpt2")
vocab_size = enc.n_vocab
print(f"Vocab size: {vocab_size}")

full_tokens = torch.tensor(enc.encode_ordinary(raw_text), dtype=torch.long)

n = int(0.9 * len(full_tokens))
train_data = full_tokens[:n]
val_data = full_tokens[:n]

def get_batch(split):
  data = train_data if split == 'train' else val_data
  ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))
  x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])
  y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])
  x, y = x.to(DEVICE), y.to(DEVICE)
  return x, y

# Core Transformer Architecture (Karpathy-style)

class SelfAttentionHead(nn.Module):
  def __init__(self, head_size):
    super().__init__()
    self.key = nn.Linear(N_EMBD, head_size, bias=False) # what kind of info I contain?
    self.query = nn.Linear(N_EMBD, head_size, bias=False) # what am I looking for?
    self.value = nn.Linear(N_EMBD, head_size, bias=False) # what content do I actually contain?

    self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE))) # causal mask as buffer (so on gpu but not trained as weight)
    self.dropout = nn.Dropout(DROPOUT) # to prevent overfitting

  def forward(self, x):
    B, T, C = x.shape # Batch, Time, Channels

    k = self.key(x) # (B, T, head_size)
    q = self.query(x) # (B, T, head_size)

    wei = q @ k.transpose(-2, -1)

    # scaled attention to prevent affinities from getting too large
    wei = wei * C ** -0.5

    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))

    wei = F.softmax(wei, dim=-1)
    wei = self.dropout(wei)

    v = self.value(x)

    out = wei @ v # use probabilities to weigh the values
    return out

class MultiHeadAttention(nn.Module):
  def __init__(self, num_heads, head_size):
    super().__init__()
    self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])
    self.proj = nn.Linear(N_EMBD, N_EMBD)
    self.dropout = nn.Dropout(DROPOUT)

  def forward(self, x):
    out = torch.cat([h(x) for h in self.heads], dim=-1) # concat results of parallel self attention heads
    out = self.dropout(self.proj(out))
    return out

class FeedForward(nn.Module):
  def __init__(self, n_embd):
    super().__init__()
    # simple MLP
    self.net = nn.Sequential(
        nn.Linear(n_embd, 4 * n_embd), # 4x expansion for more thinking capacity
        nn.ReLU(),
        nn.Linear(4 * n_embd, n_embd),
        nn.Dropout(DROPOUT)
    )

  def forward(self, x):
    return self.net(x)

class Block(nn.Module):
  def __init__(self, n_embd, n_head):
    super().__init__()
    head_size = n_embd // n_head
    self.sa = MultiHeadAttention(n_head, head_size)
    self.ffwd = FeedForward(n_embd)
    self.ln1 = nn.LayerNorm(n_embd)
    self.ln2 = nn.LayerNorm(n_embd)

  def forward(self, x):
    x = x + self.sa(self.ln1(x)) # residual connections to allow gradients to skip the layer
    x = x + self.ffwd(self.ln2(x))
    return x

# Speculative Decoding

class ParallelDecodingGPT(nn.Module):
  """GPT with auxiliary heads for blockwise parallel decoding"""
  def __init__(self):
    super().__init__()
    self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD) # token meanings
    self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD) # position meanings (for word order)
    self.blocks = nn.Sequential(*[Block(N_EMBD, N_HEAD) for _ in range(N_LAYER)])
    self.ln_f = nn.LayerNorm(N_EMBD)

    # standard LM head to predict next token t+1
    self.lm_head = nn.Linear(N_EMBD, vocab_size)

    # spec heads: predict t+2, t+3, t+4 tokens
    self.spec_heads = nn.ModuleList([
        nn.Linear(N_EMBD, vocab_size, bias=False)
        for _ in range(NUM_SPEC_HEADS)
    ])

    self.apply(self._init_weights)

  def _init_weights(self, module):
    if isinstance(module, nn.Linear):
      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
      if module.bias is not None:
        torch.nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

  def forward(self, idx, targets=None):
    B, T = idx.shape
    tok_emb = self.token_embedding_table(idx)
    pos_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))
    x = tok_emb + pos_emb # combine token identity & position

    # run the body
    x = self.blocks(x)

    x = self.ln_f(x)

    # standard prediction
    logits = self.lm_head(x)

    # speculative predictions (all heads run in parallel on same x)
    spec_logits = [head(x) for head in self.spec_heads]

    loss = None
    if targets is not None:
      # standard loss
      loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))

      # spec loss
      for k, s_logits in enumerate(spec_logits):
        offset = k + 1
        if T > offset:
          s_logits_crop = s_logits[:, :-offset, :].reshape(-1, vocab_size)
          targets_crop = targets[:, offset:].reshape(-1)

          loss += DECAY_COEFF ** offset * F.cross_entropy(s_logits_crop, targets_crop)

    return logits, spec_logits, loss

  def generate_speculative(self, idx, max_new_tokens):
    # idx is (B, T) array of indices in the current context
    generated = idx

    while generated.shape[1] < max_new_tokens + idx.shape[1]:
      # crop context if too long
      cond_idx = generated[:, -BLOCK_SIZE:]

      # Draft phase
      with torch.no_grad():
        logits, spec_logits, _ = self(cond_idx)

      next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)

      candidates = [next_token] # always take the correct token
      for s_log in spec_logits:
        cand = torch.argmax(s_log[:, -1, :], dim=-1, keepdim=True)
        candidates.append(cand)

      # put together the draft [t+1, t+2, t+3, ...]
      draft_seq = torch.cat(candidates, dim=1)

      # Verify phase
      verification_input = torch.cat([cond_idx, draft_seq], dim=1) # append draft to context

      if verification_input.shape[1] > BLOCK_SIZE:
        verification_input = verification_input[:, -BLOCK_SIZE:]

      with torch.no_grad():
        # run model again (verify pass)
        verify_logits, _, _ = self(verification_input)

      # Accept/Reject phase
      accepted_tokens = [draft_seq[0, 0].item()] # accept 1st token awlays

      draft_start = verification_input.shape[1] - draft_seq.shape[1]

      for i in range(len(candidates)-1):
        guessed = draft_seq[0, i+1].item()

        check_idx = draft_start + i
        actual = torch.argmax(verify_logits[0, check_idx, :]).item()

        if guessed == actual:
          accepted_tokens.append(guessed)
        else:
          accepted_tokens.append(actual)
          break # stop accepting future drafts, they're based on wrong premise

      new_chunk = torch.tensor(accepted_tokens, device=DEVICE).unsqueeze(0)
      generated = torch.cat([generated, new_chunk], dim=1)

    return generated

  def generate_standard(self, idx, max_new_tokens):
    for _ in range(max_new_tokens):
      idx_cond = idx[:, -BLOCK_SIZE:]
      logits, _, _ = self(idx_cond)
      logits = logits[:, -1, :]
      probs = F.softmax(logits, dim=-1)
      idx_next = torch.argmax(logits, dim=-1, keepdim=True)
      idx = torch.cat((idx, idx_next), dim=1)
    return idx

model = ParallelDecodingGPT().to(DEVICE)
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

print(f"Pre-training the base model")

model.train()
for i in range(PRETRAIN_STEPS):
  xb, yb = get_batch('train')
  logits, spec_logits, total_loss = model(xb, yb)
  main_loss = F.cross_entropy(logits.view(-1, vocab_size), yb.view(-1))

  optimizer.zero_grad(set_to_none=True)
  main_loss.backward()
  optimizer.step()
  if i % 100 == 0:
    print(f"Step {i}: Loss {main_loss.item(): .4f}")

# now freeze model
print("Freeze pre-trained model & main head")
for param in model.token_embedding_table.parameters():
  param.requires_grad = False
for param in model.position_embedding_table.parameters():
  param.requires_grad = False
for param in model.blocks.parameters():
  param.requires_grad = False
for param in model.ln_f.parameters():
  param.requires_grad = False

for param in model.lm_head.parameters():
  param.requires_grad = False

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"We are training only {trainable_params/1e6:.2f}M parameters out of {total_params/1e6:.2f}M, or {100 * trainable_params/total_params:.2f}% of the network.")

# fine tuning the speculative heads
print("Fine-tuning speculative heads")
for i in range(HEAD_TRAIN_STEPS):
  xb, yb = get_batch('train')

  logits, spec_logits, loss = model(xb, yb)

  optimizer.zero_grad(set_to_none=True)
  loss.backward()
  optimizer.step()

  if i % 100 == 0:
    print(f"Step {i}: Spec loss {loss.item(): .4f}")

# Benchmarking Inference Speedup
print("Benchmarking Inference Speedup")
model.eval()

ctx = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)
TOKENS_TO_GEN = 200

# Standard inference
start_t = time.time()
result = model.generate_standard(ctx, TOKENS_TO_GEN)
if DEVICE == 'cuda':
  torch.cuda.synchronize()
end_t = time.time()
baseline_time = end_t - start_t
print(f"Standard decoding: {baseline_time:.4f}s ({TOKENS_TO_GEN/baseline_time: .2f} tokens/s)")
print(f"Result: {result}")

# Speculative inference
start_t = time.time()
result = model.generate_speculative(ctx, TOKENS_TO_GEN)
if DEVICE == 'cuda':
  torch.cuda.synchronize()
end_t = time.time()
spec_time = end_t - start_t
print(f"Speculative decoding: {spec_time:.4f}s ({TOKENS_TO_GEN/spec_time:.2f} tokens/s)")
print(f"Result: {result}")

print(f"Speedup Factor: {baseline_time/spec_time:.2f}x")